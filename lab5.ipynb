{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33e32b63",
   "metadata": {},
   "source": [
    "# Lab 5: Web Scraping with Python\n",
    "## ENGL 6701 Spring 2024\n",
    "\n",
    "Contact:\n",
    "Lindsay Thomas, lthomas@cornell.edu\n",
    "\n",
    "For more information about this lab and the context in which it was used in ENGL 6701, see [the Lab 5 page on the ENGL 6701 Spring 2024 course website](https://lindsaythomas.net/engl6701s24/labs/lab-5.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052ff683",
   "metadata": {},
   "source": [
    "### 0. Create and Save a Notes Document\n",
    "\n",
    "As with Lab 4, you will want to create and save a notes document before beginning this lab. Starting with section 2, you will be asked to write some simple code. As you complete these portions of the lab, copy and paste the code you write into your notes document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1881ab90",
   "metadata": {},
   "source": [
    "Since we are once again using Binder to work with this notebook, please remember that none of the changes you make will be saved after your browser session. This includes outputs that are displayed to the notebook, as well as things like inserting filenames. When you shut this tab down or your laptop loses its connection to the internet or server, you will need to restart this notebook, and it will be like restarting from the beginning. That's why you should copy and paste the code you write into a notes document. You should also know that you may need to rerun cells from the beginning of the notebook to get later sections of the notebook to function correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb876676",
   "metadata": {},
   "source": [
    "Sections 1 and 2 of this notebook are drawn from the [\"Web Scraping -- Part 1\" section of Chapter 4, \"Data Collection,\"](https://melaniewalsh.github.io/Intro-Cultural-Analytics/04-Data-Collection/02-Web-Scraping-Part1.html) from Melanie Walsh's free online textbook, *Introduction to Cultural Analytics & Python*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d1d08b",
   "metadata": {},
   "source": [
    "### 1. What is Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072b7ae",
   "metadata": {},
   "source": [
    "To illustrate what web scraping is and how it's useful, let's look at a dataset collected by Cornell CIS faculty Cristian Danescu-Niculescu-Mizil and Lillian Lee over ten years ago now. These researchers utilized this corpus, the [Cornell Movie Dialogues Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), in their paper [\"Chameleons in Imagined Conversations\"](https://www.cs.cornell.edu/~cristian/papers/chameleons.pdf). To create this dataset, these researchers scraped movie scripts from various websites; they kept track of each url they used. Let's look at these urls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f07a2d",
   "metadata": {},
   "source": [
    "First, we're going to import [pandas](https://pypi.org/project/pandas/), which is a package for handling tabular data within Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31068d8d",
   "metadata": {},
   "source": [
    "Next, we're going to read in a spreadsheet that lists the titles and urls where the researchers found each script. In Python-speak, we're using `pandas` to create a DataFrame to store this tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = pd.read_csv(\"raw_script_urls.csv\", delimiter='\\t', encoding='utf=8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc5247",
   "metadata": {},
   "source": [
    "Display the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bd7da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d6ad5",
   "metadata": {},
   "source": [
    "We can see from looking at this output that each row in this DataFrame is a movie, and the script url is listed in the third column. We could use this information to manually navigate to each listed url and copy/paste each script into a txt file, but that method would be labor-intensive and we run the risk of losing information that isn't displayed on the web page itself or that is structured in a weird way but that may be useful to us. So instead, we're going to programmatically access the scripts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127659d",
   "metadata": {},
   "source": [
    "#### Request and Response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74b8664",
   "metadata": {},
   "source": [
    "When you type in a url to the address bar in your browser, you are sending an HTTP **request** for a web page. The server that stores that web page then sends back a **response**, which is the web page data that your browser renders.\n",
    "\n",
    "We can use a Python library called [requests](https://requests.readthedocs.io/en/latest/) to programmatically access the data sent via those responses. Let's import requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef25ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972d5df8",
   "metadata": {},
   "source": [
    "#### Get HTML Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5318122f",
   "metadata": {},
   "source": [
    "Head over to <http://www.scifiscripts.com/scripts/Ghostbusters.txt> in your browser. When we look at this webpage, we can see that it's just a plain-text file that contains the script for the movie *Ghostbusters*. \n",
    "\n",
    "We can capture the data contained in that plain-text file by using the `.get()` function associated with the requests library. We will store what we get in a variable called `response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a77c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://www.scifiscripts.com/scripts/Ghostbusters.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ff2f3",
   "metadata": {},
   "source": [
    "However, if we check this variable, we see that it just gives us the [HTTP response code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status), which tells us if the request was successful or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbe13e9",
   "metadata": {},
   "source": [
    "In this case, it was: \"200\" is a successful response.\n",
    "\n",
    "But let's see what happens if we change the url in our request to a webpage that doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90301581",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_response = requests.get(\"http://www.scifiscripts.com/scripts/Ghostboogers.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53044914",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023624bf",
   "metadata": {},
   "source": [
    "404, on the other hand, is a common \"Page Not Found\" error. If you head over to <http://www.scifiscripts.com/scripts/Ghostboogers.txt>, you'll see what this looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0151d09c",
   "metadata": {},
   "source": [
    "#### Extract Text from a Web Page\n",
    "\n",
    "In order to actually read the text data in our response, we need to use `.text`, which we will save in a variable called `html_string`. We will use the data stored in the `response` variable we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9be96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_string = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2f339b",
   "metadata": {},
   "source": [
    "Now, if we print `html_string`, we will be able to see the text data stored on the screenplay's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe725c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(html_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2bb6fb",
   "metadata": {},
   "source": [
    "#### Extract Text from Multiple Web Pages\n",
    "\n",
    "But how could we grab the screenplay for every movie in the DataFrame of movie scripts we created above? We can write a function that will do this. To demonstrate this, let's first create a smaller version of our movie script dataframe, one including only 10 scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cae89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this line of code, we are using Python's built-in indexing functionality to tell the computer to take \n",
    "# only the first 10 rows in the urls DataFrame we created above.\n",
    "sample_urls = urls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eca086",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9be5c9",
   "metadata": {},
   "source": [
    "Now, we need to write a function that 1) gets the data from each web page; and 2) stores the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95dd5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we define the function and tell it to act on a single url\n",
    "def scrape_screenplay(url):\n",
    "    # then we get the data from that url\n",
    "    response = requests.get(url)\n",
    "    # then we store the text data in a variable called `html_string`\n",
    "    html_string = response.text\n",
    "    # then we return the `html_string` variable as output\n",
    "    return html_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90cd5e",
   "metadata": {},
   "source": [
    "Then we apply this function to the “script_url” column of the DataFrame and create a new column for the resulting extracted text. Pandas makes this easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc2cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code says: Apply the 'scrape_screenplay' function to each row of the 'sample_urls' dataframe.\n",
    "# use whatever is in the 'script_url' column as the input. \n",
    "# then, store the output for each row in a new column titled 'text'.\n",
    "sample_urls['text'] = sample_urls['script_url'].apply(scrape_screenplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b0304",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42005993",
   "metadata": {},
   "source": [
    "If we print out every row in the column, we can see that we successfully extracted text for each URL (though some of these URLs returned 404 errors). This text is encoded in HTML (hence the tags you see in the 'text' column above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adcb8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this for loop says: for each row in the 'sample_urls' dataframe, print out the value of the 'text' column\n",
    "for text in sample_urls['text']:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1849d6",
   "metadata": {},
   "source": [
    "### 2. Working with HTML Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aa7a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code block will import all of the packages we need for this section.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d1d375",
   "metadata": {},
   "source": [
    "Most pages we will want to scrape, however, won't be as simple as our moive script urls. What's more, we will sometimes want to collect only some information included on a page, or we will want to restructure the data included in a web page for our own ends. To do this, we need to be familiar with how to programmatically extract specific pieces of information from a web page. We will use the Python library [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/), which parses HTML (HyperText Markup Language) documents, to do this (last week's lab also used BeautifulSoup; it can also parse TEI)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc58f02",
   "metadata": {},
   "source": [
    "To get a handle on using BeautifulSoup to parse HTML, we're going to examine a toy website made by the poet, programmer, and professor Allison Parrish for purposes of teaching BeautifulSoup. \n",
    "\n",
    "Here's the website: <https://static.decontextualize.com/kittens.html>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bee4020",
   "metadata": {},
   "source": [
    "#### Scraping the Kittens Web Page\n",
    "\n",
    "Let's use the requests library to scrape this Kitten TV website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3359320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we get the data on the website\n",
    "response = requests.get(\"http://static.decontextualize.com/kittens.html\")\n",
    "\n",
    "# then we specify we want to see the text data\n",
    "html_string = response.text\n",
    "\n",
    "# then we print out what we got\n",
    "print(html_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce5c7f",
   "metadata": {},
   "source": [
    "If we examine the above output, we can see that HTML uses \"tags\" to represent different elements of the page, such as the `<h1>` tag, or the main header tag, which marks the first line we see on the kittens web page. HTML tags usually, though not always, also require closing tags. For example, the main header \"Kittens and the TV Shows They Love\" is surrounded by an \"opening\" `<h1>` tag and a \"closing\" `</h1`> tag.\n",
    "\n",
    "You can see an alphabetized list of HTML tags on this page: <https://www.w3schools.com/tags/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceda3c88",
   "metadata": {},
   "source": [
    "HTML elements sometimes come with even more information inside a tag, such as attributes, classes, and IDs. This information will often consist of a keyword (like class or id) followed by an equals sign = and a further descriptor such as `<div class=\"kitten\">` or `<ul class=\"tvshows\">`.\n",
    "\n",
    "We need to know about tags as well as attributes, classes, and IDs because this is how we’re going to extract specific HTML data with BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c0acb",
   "metadata": {},
   "source": [
    "#### Using BeautifulSoup to Extract Data from HTML Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a282c72",
   "metadata": {},
   "source": [
    "First, import BeautifulSoup if you haven't already done so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae59d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f20be9c",
   "metadata": {},
   "source": [
    "To make a BeautifulSoup document, we call `BeautifulSoup()` with two parameters: the `html_string` from our HTTP request above and the kind of parser that we want to use, which will always be `html.parser` when we're dealing with HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced8067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, get the data from the kittens website\n",
    "response = requests.get(\"http://static.decontextualize.com/kittens.html\")\n",
    "\n",
    "# then, specify that we want just the text\n",
    "html_string = response.text\n",
    "\n",
    "# finally, call `BeautifulSoup()` and tell it to parse the `html_string` variable using the html parser\n",
    "document = BeautifulSoup(html_string, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b1a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3d11c0",
   "metadata": {},
   "source": [
    "The above output looks pretty similar to what we got above when we were just using requests. However, now we can use BeautifulSoup to further parse this data and extract specific elements. We will use `find()` for this.\n",
    "\n",
    "Run the two following cells. How are their outputs different and why are they different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82930a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find(\"h1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c1646",
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find(\"h1\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbe6bdd",
   "metadata": {},
   "source": [
    "We can also use `find()` to grab the first tag that matches the specific element we request. For example, if we wanted to find the first image in the Kittens web page, we could run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f9f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find(\"img\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4458030d",
   "metadata": {},
   "source": [
    "#### Now You Try It: Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc9197",
   "metadata": {},
   "source": [
    "What if we wanted to find the data embedded in the first [`<li>` tag](https://www.w3schools.com/tags/tag_li.asp) above? Write the code to do this below, and then copy and paste it into your notes document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0edf6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85436cd7",
   "metadata": {},
   "source": [
    "#### Using BeautifulSoup to Extract Multiple HTML Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bebcd4",
   "metadata": {},
   "source": [
    "Notice how the above image file is the first value embedded within an `<img>` tag in our HTML document. But what if we wanted to find *all* of the images on the website?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a12b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find_all(\"img\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda817a6",
   "metadata": {},
   "source": [
    "We can also specify that we want to find elements with specific attributes. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd00803",
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find_all(\"div\", attrs={\"class\": \"kitten\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef161cb",
   "metadata": {},
   "source": [
    "#### Now You Try It: Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d539f8ce",
   "metadata": {},
   "source": [
    "Let's find all of the data in `<ul>` tags whose `class=\"tvshows\"`. Write the code to do this below, and then copy and paste it into your notes document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813fb322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4979d839",
   "metadata": {},
   "source": [
    "#### Using For Loops to Extract Multiple HTML Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd26ef",
   "metadata": {},
   "source": [
    "Ok, now let's try to extract text from all of `<h2>` elements in our document. First, we find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8338a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find_all(\"h2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51ab398",
   "metadata": {},
   "source": [
    "Great, we can see from the above output that there are 2 different `<h2>` values in the document. Let's extract just the text (\"Fluffy,\" \"Monsieur Whiskeurs\") from both.\n",
    "\n",
    "(Note: The code in the cell below is supposed to cause an error.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7ccf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find_all(\"h2\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0c81da",
   "metadata": {},
   "source": [
    "Whoops, looks like we need a `for` loop so that we can cycle through the value for each `<h2>` tag and extract the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d5ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, find all the h2 values and store them in the `all_h2_headers` variable\n",
    "all_h2_headers = document.find_all(\"h2\")\n",
    "\n",
    "all_h2_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6be84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, create an empty list where we will put the text we extract\n",
    "h2_headers = []\n",
    "\n",
    "# now, create a for loop in which, for each header in `all_h2_headers`, we grab the text, \n",
    "# put it in a variable called `header_contents`, then append that value to our `h2_headers` list\n",
    "for header in all_h2_headers:\n",
    "    header_contents = header.text\n",
    "    h2_headers.append(header_contents)\n",
    "\n",
    "# finally, print out what's in `h2_headers`\n",
    "h2_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308aa7fd",
   "metadata": {},
   "source": [
    "#### Now You Try It: Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442cd49c",
   "metadata": {},
   "source": [
    "Let's write a `for` loop that will allow us to put all of the TV show names into a list. Write the code to do this in the below cells, and then copy and paste it into your notes document once it's working.\n",
    "\n",
    "First, find all of the TV show names. What element do we need to `find_all()` of?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5832cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589aeae",
   "metadata": {},
   "source": [
    "Then, create an empty list to store the show names, and write a `for` loop that cycles through each element and extracts the text of the TV show names. Hint: Your code should look very similar to the `for` loop above -- you might just change the variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4013d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb01556",
   "metadata": {},
   "source": [
    "#### Want an Additional Challenge?: Question 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f8a0a8",
   "metadata": {},
   "source": [
    "Write code that will extract all of the links to each TV show's IMDB page and put those links into a list. We want just the links here, not any of the tags. As before, write the code to do this in the below cell, and then copy and paste it into your notes document once it's working.\n",
    "\n",
    "Hint: We need to use a method other than `.text` to accomplish this. This page might be helpful: <https://www.educative.io/answers/beautiful-soup-get-href>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba2368d",
   "metadata": {},
   "source": [
    "### 3. Scraping Goodreads\n",
    "\n",
    "The code in this section is drawn from [Adesua Ayomitan's Goodreads webscraping notebook](https://github.com/Adesuaayo/goodreads_webscraper/blob/main/Goodreads_webscraper.ipynb). The section is inspired by Melanie Walsh's and Maria Antoniak's article [\"The Goodreads 'Classics'\"](https://culturalanalytics.org/article/22221-the-goodreads-classics-a-computational-study-of-readers-amazon-and-crowdsourced-amateur-criticism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face9c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code block will import all of the packages we need for this section if you haven't already\n",
    "# loaded them.\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3538e993",
   "metadata": {},
   "source": [
    "We're now going to move on to scraping a more complicated website: Goodreads. However, we're still going to keep it as simple as possible, and we're going to scrape the top 100 books that Goodreads users have shelved as \"classics.\"\n",
    "\n",
    "Specifically, we're going to scrape this page: <https://www.goodreads.com/search?q=classics&qid=>.\n",
    "\n",
    "If you head to that page and take a look, you'll see that it lists the \"top\" books users have shelved as \"classics,\" based on users' ratings and the number of ratings. We can see that Goodreads displays 10 books per page, and that we can page through well over 100 pages of results.\n",
    "\n",
    "We want to collect the top 100 books, so that means collecting data from the first 10 pages of results. And we want to collect the following information about each book:\n",
    "- title\n",
    "- author\n",
    "- average rating\n",
    "- year published\n",
    "- number of editions (available on Goodreads)\n",
    "\n",
    "If we look at the search results page again, we can see that that information is included in the text that appears about each book. \n",
    "\n",
    "So now we need to figure out how to scrape it using BeautifulSoup. To do that, we first need to figure out what tags are being used to encode the data we want to collect from these pages. We could, as we did before, grab all of the data and extract the text to see the tags. However, for a page like this, that would be pretty unwieldy. Instead, we're going to visually inspect the search results page using our browsers. I recommend using Chrome or Firefox for this part of the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d07c85",
   "metadata": {},
   "source": [
    "#### Inspecting the Search Results Page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57d3a2",
   "metadata": {},
   "source": [
    "Head over to the \"classics\" search results page: <https://www.goodreads.com/search?q=classics&qid=>. Right- or control-click on the page, and select \"Inspect\" on the menu that comes up. An inspector panel should pop up. This panel reveals the HTML structuring the page. Mousing over each line of the HTML highlights that area of the web page.\n",
    "\n",
    "Mouse over each line of HTML until the table displaying the search results is highlighted. It's defined by the `<tbody>` tag (you may need to select the little arrows next to lines to reveal hierarchically-nested elements). This is how it looks in Firefox:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92458e90",
   "metadata": {},
   "source": [
    "![inspect tbody tag in firefox](https://github.com/lcthomas/engl6701s24-lab45/firefox.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad108de",
   "metadata": {},
   "source": [
    "#### Extracting the Whole Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6960c1c5",
   "metadata": {},
   "source": [
    "Let's try to extract the information in this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda8936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we define the url we want to scrape\n",
    "url = \"https://www.goodreads.com/search?q=classics&qid=\"\n",
    "\n",
    "# next, we extract just the text from the url\n",
    "response = requests.get(url).text\n",
    "\n",
    "# then, we call BeautifulSoup to parse the text\n",
    "soup = BeautifulSoup(response, \"html5lib\")\n",
    "\n",
    "# finally, we find the information in the <tbody> tag and store it in the variable 'table'\n",
    "table = soup.find(\"tbody\")\n",
    "# table = soup.find_all(\"tbody\")[0]\n",
    "\n",
    "# then we print 'table'\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d62601",
   "metadata": {},
   "source": [
    "Ok, we're on the right track, but this is still messy and hard to understand. Let's try to locate and extract information from just one row of this table instead of the whole thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038cb2e",
   "metadata": {},
   "source": [
    "Head back to the \"classics\" search results page that you are inspecting in your browser (https://www.goodreads.com/search?q=classics&qid=, right- or control-click and select Inspect). The next level under the `<tbody>` tag is the `<tr>` tag; in HTML `<tr>` designates a table row. Let's extract the text of the first table row (for *Sense and Sensibility*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da7a81d",
   "metadata": {},
   "source": [
    "#### Extracting Just One Row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af242880",
   "metadata": {},
   "source": [
    "#### Now You Try It: Question 4\n",
    "\n",
    "Looking at the above code, what do we need to do next to extract just one row from this table? Store this information in a variable named `first_row`. Write the code to do this in the below cell, and then copy and paste it into your notes document once it's working.\n",
    "\n",
    "Hint: Code that will answer this question is included in the large code block in section 4 of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25841666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea29eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at what you stored in 'first_row'\n",
    "first_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06409fd1",
   "metadata": {},
   "source": [
    "Ok, now we've narrowed it down to just one row of the table, but there's still way more information here than we want. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a718e",
   "metadata": {},
   "source": [
    "#### Extracting Part of Just One Row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6fc4e3",
   "metadata": {},
   "source": [
    "Looking back at the Goodreads results page using our browser inspector, we can see that each row is also composed of cells that contain specific information. These cells are encoded with the `<td>` tag. If we mouse over the first `<td>` tag, we see that it refers to the image of the book cover. We don't want that. But if we mouse over hte second `<td>` tag, we see that it highlights the second cell of the row, which does contain all the information we want. Let's extract just the information in the *second* cell of this first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2927991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this line, we are using `.find_all()` to grab all of the <td> values stored in the 'first_row' variable,\n",
    "# but we are also using Python's built-in indexing functionality to select just the information stored in the\n",
    "# second <td> tag (Python indexing begins at 0).\n",
    "second_cell = first_row.find_all(\"td\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5c115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to see the value of 'second_cell'\n",
    "second_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55765959",
   "metadata": {},
   "source": [
    "Still a lot of stuff we don't need here, but we can now zero in on what we do want: the book title, author, average rating, year published, and number of editions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be50d6",
   "metadata": {},
   "source": [
    "#### Now You Try It: Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19904b72",
   "metadata": {},
   "source": [
    "Take a look at the below code. In your notes document, write comments that describe what each line of code is doing, except for the `print` lines. If you're working directly in this notebook on your own computer, you can just write the comments above each line (make sure to use a hashtag at the beginning of the line to set them off as comments!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1925445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = second_cell.find(\"a\").find(\"span\").text\n",
    "print(title)\n",
    "\n",
    "author = second_cell.find(\"a\", class_=\"authorName\").text\n",
    "print(author)\n",
    "\n",
    "all_ratings = second_cell.find_all('span', class_ = 'minirating')\n",
    "print(all_ratings)\n",
    "\n",
    "year_info = second_cell.find(\"span\", class_=\"greyText smallText uitext\").text.split()\n",
    "print(year_info)\n",
    "\n",
    "editions = second_cell.find(\"span\", class_=\"greyText smallText uitext\").text.split()[-2]\n",
    "print(editions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b761644c",
   "metadata": {},
   "source": [
    "As you can see from the printed values above, there is a bit more processing we need to do to isolate the average rating, number of ratings, and year published values, but we are nearly there now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649cc386",
   "metadata": {},
   "source": [
    "### 4. Putting It All Together (and then some)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb86cc1b",
   "metadata": {},
   "source": [
    "Now that we've walked through the process of extracting (most of) the information we want for just one book, let's put it all together and see what it looks like to extract this information not only for *all* of the results on the first page, but also for the next 9 pages. Read through the five code blocks below and do your best to understand what's happening in each line. The second code block will take several moments to run; wait until it completes running before moving on to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6ca509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we define some empty lists where we will place the values we extract\n",
    "book_titles = []\n",
    "authors = []\n",
    "avg_ratings = []\n",
    "ratings = []\n",
    "published_years = []\n",
    "editions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b5e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the big one!\n",
    "\n",
    "# first, we define how many pages we want to scrape data from\n",
    "pages_to_scrape = 10\n",
    "\n",
    "# specify the delay between requests in seconds (e.g., 2 seconds)\n",
    "# the delay mimics the behavior of a human paging through results so that the goodreads servers don't\n",
    "# shut you down\n",
    "request_delay = 3\n",
    "\n",
    "# for each page\n",
    "for page in range(1, pages_to_scrape + 1):\n",
    "    \n",
    "    # Construct the url for the current page\n",
    "    url = \"https://www.goodreads.com/search?page=\" + str(page) + \"&q=classics&qid=mXUTlUsh6g&search_type=books&tab=books&utf8=✓\"\n",
    "   \n",
    "    # wrap the code in a `try except` loop to help with error handling and so that the whole thing doesn't\n",
    "    # shut down if it encounters errors\n",
    "    try:\n",
    "        # send an http .get() request to the url constructed above \n",
    "        response = requests.get(url).text\n",
    "\n",
    "        # Parse the html content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response, \"html5lib\")\n",
    "    \n",
    "        # check for server errors or maintenance\n",
    "        # if there's a server error, inform the user and then skip that page\n",
    "        if soup.title and \"service unavailable\" in soup.title.text.lower():\n",
    "            print(f\"Server error on page {page}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # select the table containing the list of books\n",
    "        table = soup.find(\"tbody\")\n",
    "\n",
    "        # for each row\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cells = row.find_all(\"td\")[1]\n",
    "\n",
    "            # extract book title\n",
    "            title = cells.find(\"a\").find(\"span\").text\n",
    "            # append the title to the 'book_titles' list we created above\n",
    "            book_titles.append(title)\n",
    "\n",
    "            # extract author's name\n",
    "            author = cells.find(\"a\", class_=\"authorName\").text\n",
    "            # append author's name to the 'authors' list we created above\n",
    "            authors.append(author)\n",
    "            \n",
    "\n",
    "            # extract ratings\n",
    "            all_ratings = cells.find_all('span', class_ = 'minirating')\n",
    "            # do some string operations and regular expressions work to isolate the average\n",
    "            # rating value and append it to the 'avg_ratings' list\n",
    "            all_ratings_text = all_ratings[0].text.strip()\n",
    "            pattern_2 = re.compile(r\"(\\d\\.?\\d*)\\savg\")\n",
    "            avg_ratings.append(pattern_2.search(all_ratings_text).group(1))\n",
    "\n",
    "            # extract number of ratings from the data extracted above and stored in 'all_ratings_text'\n",
    "            # do it using regular expressions\n",
    "            pattern_4 = re.compile(r\"(\\d\\,?\\d*) rating\")\n",
    "            ratings_matches = pattern_4.search(all_ratings_text)\n",
    "            ratings.append(ratings_matches.group(1) if ratings_matches else 0)  \n",
    "\n",
    "            # extract published year, handling cases where it may not be in the expected format\n",
    "            year_info = cells.find(\"span\", class_=\"greyText smallText uitext\").text.split()\n",
    "            year = None\n",
    "            for item in year_info:\n",
    "                if item.isdigit() and len(item) == 4:\n",
    "                    year = item\n",
    "                    break\n",
    "            if year:\n",
    "                published_years.append(year)   # append to the 'published_years' list\n",
    "            else:\n",
    "                published_years.append(0)  # handle cases where year is not found\n",
    "\n",
    "            # extract edition information and append to the 'editions' list\n",
    "            edition = cells.find(\"span\", class_=\"greyText smallText uitext\").text.split()[-2]\n",
    "            editions.append(edition)\n",
    "\n",
    "        # sleep to add a delay between requests\n",
    "        time.sleep(request_delay)\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # handle http request errors (e.g., connection issues)\n",
    "        print(f\"Error on page {page}: {e}\")\n",
    "\n",
    "    except IndexError as e:\n",
    "        # handle \"list index out of range\" error\n",
    "        print(f\"Index error on page {page}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # handle other unexpected errors\n",
    "        print(f\"Unexpected error on page {page}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d93c12",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "\n",
    "There are two `for` loops in the code block above (well, there are actually three, but let's just look at the first two): one begins on line 12, and one on line 36. What is the code looping through in each one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b73e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after scraping all pages, we create a dictionary to store the collected data\n",
    "data = {\n",
    "    \"Title\": book_titles,\n",
    "    \"Author\": authors,\n",
    "    \"Average Rating\": avg_ratings,\n",
    "    \"Rating\": ratings,\n",
    "    \"Year Published\": published_years,\n",
    "    \"Editions\": editions\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we use pandas to create a dataframe to display the data\n",
    "goodreads = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f061bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first and last five rows of the dataframe\n",
    "goodreads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9cc022",
   "metadata": {},
   "source": [
    "If you're running this notebook on your own computer, you can uncomment the below line of code to save this dataframe as a `.csv` file to your computer. It will save to the same directory where you stored this notebook file. Then, you can open up this file using Excel on your own computer. I've also placed a copy of this file in the lab 5 folder on our Canvas site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a40cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# goodreads.to_csv(\"Goodreads_classics_top100.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
